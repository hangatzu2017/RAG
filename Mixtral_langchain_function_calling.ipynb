{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hangatzu2017/RAG/blob/main/Mixtral_langchain_function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NB:** Per eseguire questo notebook è necessario possedere un account a pagamento di colab o un jupyter on premise.\n",
        "Quanto segue in questo notebook è un tentativo di utilizzo di \"function calling\" su LLM open source."
      ],
      "metadata": {
        "id": "MJCF9SapO_QR"
      },
      "id": "MJCF9SapO_QR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installazione delle librerie necessarie e Download del modello"
      ],
      "metadata": {
        "id": "8-laHzZmM4hp"
      },
      "id": "8-laHzZmM4hp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a19066a-f346-47c0-8313-9b7851dc1bb1",
      "metadata": {
        "id": "1a19066a-f346-47c0-8313-9b7851dc1bb1"
      },
      "outputs": [],
      "source": [
        "!pip install -U -qq transformers accelerate exllamav2 langchain\n",
        "!apt-get update && apt-get install git-lfs\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b4d099-a6be-4993-b866-dcba00d956b0",
      "metadata": {
        "id": "37b4d099-a6be-4993-b866-dcba00d956b0"
      },
      "outputs": [],
      "source": [
        "# Download del modello Mixtral 8x7B Instruct (Quantizzato 2bit)\n",
        "!wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selezione del tipo di accellerazione da utilizzare con LLama-cpp-python ed installazione della stessa."
      ],
      "metadata": {
        "id": "vq2ee_j4NySg"
      },
      "id": "vq2ee_j4NySg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6a22e2-a396-428e-a3ab-da8db042972f",
      "metadata": {
        "id": "9a6a22e2-a396-428e-a3ab-da8db042972f"
      },
      "outputs": [],
      "source": [
        "# Base ctransformers senza accellerazione GPU\n",
        "#!pip install llama-cpp-python\n",
        "\n",
        "# Con accellerazione NVidia CUDA ( Questo fa per noi :) )\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "\n",
        "# Oppure con accellerazione OpenBLAS\n",
        "#!CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n",
        "\n",
        "# O ancora con accellerazione CLBLast\n",
        "#!CMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n",
        "\n",
        "# Oppure ancora con accellerazione AMD ROCm GPU (solo per sistemi Linux)\n",
        "#!CMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n",
        "\n",
        "# Infine con accellerazione Metal GPU per soli sistemi macOS\n",
        "#!CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681d1bdf-a891-4fd5-9452-50a23c13aac5",
      "metadata": {
        "id": "681d1bdf-a891-4fd5-9452-50a23c13aac5"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "# Parametri di configurazione del modello\n",
        "n_gpu_layers = 32  # Metal impostato ad 1 è sufficiente.\n",
        "n_batch = 512  # Dovrebbe essere compreso tra 1 e n_ctx, tieni presente la quantità di RAM del tuo Silicon Chip Apple.\n",
        "n_ctx=8192 # Il context varia a seconda del modello solitamente il valore max è dichiarato nella scheda di presentazionedel modello stesso.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# Definizione del modello\n",
        "chat_model = LlamaCpp(\n",
        "    model_path=\"/content/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    n_ctx=n_ctx,\n",
        "    f16_kv=True,  # NB: DEVE essere impostato a True, altrimenti si verificheranno problemi dopo un paio di chiamate.\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True, # Questo è parlante.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be48d65-ba48-4c34-b4f8-223a9636accb",
      "metadata": {
        "id": "3be48d65-ba48-4c34-b4f8-223a9636accb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Definizione della funzione per la lettura della temperatura nella località specificata\n",
        "def get_weather_data(coordinates):\n",
        "    \"\"\"\n",
        "    Recupera i dati meteo dall'API Open-Meteo per la latitudine e la longitudine specificate.\n",
        "\n",
        "    Argomenti:\n",
        "    coordinate (tupla): Latitudine e longitudine della località.\n",
        "\n",
        "    Restituisce:\n",
        "    float: Temperatura attuale della località.\n",
        "    \"\"\"\n",
        "    latitude, longitude = coordinates\n",
        "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
        "    params = {\n",
        "        \"latitude\": latitude,\n",
        "        \"longitude\": longitude,\n",
        "        \"current\": \"temperature_2m,wind_speed_10m\",\n",
        "        \"hourly\": \"temperature_2m,relative_humidity_2m,wind_speed_10m\",\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return f\"\"\"Temperatura(C): {response.json()[\"current\"][\"temperature_2m\"]}\"\"\"\n",
        "    else:\n",
        "        return {\n",
        "            \"error\": \"Impossibile recuperare i dati, status code: {}\".format(\n",
        "                response.status_code\n",
        "            )\n",
        "        }\n",
        "\n",
        "\n",
        "# Definizione della funzione per la lettura\n",
        "def get_coordinates_from_city(city_name):\n",
        "    \"\"\"\n",
        "    Recupera la latitudine e la longitudine per il nome della città specificata utilizzando l'API di geocodifica di Maps.co.\n",
        "\n",
        "    Argomenti:\n",
        "    city_name (str): Nome della città.\n",
        "\n",
        "    Restituisce:\n",
        "    tupla: Latitudine e longitudine della città.\n",
        "    \"\"\"\n",
        "    base_url = \"https://geocode.maps.co/search\"\n",
        "    params = {\"q\": city_name}\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if data:\n",
        "            # Supponiamo che il primo risultato sia il più rilevante\n",
        "            return data[0][\"lat\"], data[0][\"lon\"]\n",
        "        else:\n",
        "            return {\"error\": \"Non sono stati trovati dati per il nome della città specificata.\"}\n",
        "    else:\n",
        "        return {\n",
        "            \"error\": \"Impossibile recuperare i dati, status code: {}\".format(\n",
        "                response.status_code\n",
        "            )\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d6ad4a2-0b2f-4d1d-a468-87e2d838502c",
      "metadata": {
        "id": "5d6ad4a2-0b2f-4d1d-a468-87e2d838502c"
      },
      "outputs": [],
      "source": [
        "# Definizione del prompt del modello\n",
        "\n",
        "prompt_template = \\\n",
        "'''\n",
        "Function:\n",
        "def get_weather_data(coordinates):\n",
        "    \"\"\"\n",
        "    Fetches weather data from the Open-Meteo API for the given latitude and longitude.\n",
        "\n",
        "    Args:\n",
        "    coordinates (tuple): The latitude of the location.\n",
        "\n",
        "    Returns:\n",
        "    float: The current temperature in the coordinates you've asked for\n",
        "    \"\"\"\n",
        "\n",
        "Function:\n",
        "def get_coordinates_from_city(city_name):\n",
        "    \"\"\"\n",
        "    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.\n",
        "\n",
        "    Args:\n",
        "    city_name (str): The name of the city.\n",
        "\n",
        "    Returns:\n",
        "    tuple: The latitude and longitude of the city.\n",
        "    \"\"\"\n",
        "\n",
        "User Query: {query}<human_end>\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5088748c-ac0e-4332-8960-78491e69b9c0",
      "metadata": {
        "id": "5088748c-ac0e-4332-8960-78491e69b9c0"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# La nostra Domanda\n",
        "query = \"Com'è il tempo a Roma in questo momento?\"\n",
        "messages = [\n",
        "    HumanMessage(content=prompt_template.format(query=query))\n",
        "]\n",
        "\n",
        "result = chat_model.invoke(messages)\n",
        "\n",
        "# Stampa del risultato (risposta)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51abeeb3-2096-4671-8f98-910541c5449d",
      "metadata": {
        "id": "51abeeb3-2096-4671-8f98-910541c5449d"
      },
      "source": [
        "# CHAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd531dc3-1f61-4981-9ee4-f3c0827a2cac",
      "metadata": {
        "id": "fd531dc3-1f61-4981-9ee4-f3c0827a2cac"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "\n",
        "def call_function(text: str):\n",
        "    return eval(text)\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessagePromptTemplate.from_template(prompt_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63350907-4ede-4234-8e80-c89e562edcc2",
      "metadata": {
        "id": "63350907-4ede-4234-8e80-c89e562edcc2"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke(\"Quali sono la latitudine e la longitudine di Firenze?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43fd3ca9-8436-4f4a-9931-b50093e4cac4",
      "metadata": {
        "id": "43fd3ca9-8436-4f4a-9931-b50093e4cac4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}